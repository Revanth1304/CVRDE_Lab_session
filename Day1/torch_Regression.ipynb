{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Regression \n",
    "\n",
    "\n",
    "Imvolves the following steps\n",
    "\n",
    "1. Import necessary libraries\n",
    "1. Generate synthetic data for Regression case and split the dataset\n",
    "1. Load data into loader into batches\n",
    "1. Create a simple model\n",
    "1. Initialize optimizer and loss function\n",
    "1. Strat  Training \n",
    "1. Draw the plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate synthetic data for regression\n",
    "# Generate synthetic dataset (sin(x))\n",
    "torch.manual_seed(42)\n",
    "num_samples = 10000\n",
    "X_reg = torch.unsqueeze(torch.linspace(-2 * np.pi, 2 * np.pi, num_samples), dim=1)\n",
    "y_reg = torch.sin(X_reg) + 0.1 * torch.randn_like(X_reg)  # adding noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, val, test sets (70:20:10)\n",
    "split1 = int(0.7 * len(X_reg))\n",
    "split2 = int(0.9 * len(X_reg))\n",
    "X_train_reg, y_train_reg = X_reg[:split1], y_reg[:split1]\n",
    "X_val_reg, y_val_reg = X_reg[split1:split2], y_reg[split1:split2]\n",
    "X_test_reg, y_test_reg = X_reg[split2:], y_reg[split2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataloader for batches\n",
    "\n",
    "# Create DataLoader for training and validation sets\n",
    "train_dataset_reg = TensorDataset(X_train_reg, y_train_reg)\n",
    "val_dataset_reg = TensorDataset(X_val_reg, y_val_reg)\n",
    "test_dataset_reg = TensorDataset(X_test_reg, y_test_reg)\n",
    "\n",
    "train_loader_reg = DataLoader(train_dataset_reg, batch_size=64, shuffle=True)\n",
    "val_loader_reg = DataLoader(val_dataset_reg, batch_size=64)\n",
    "test_loader_reg = DataLoader(test_dataset_reg, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model for regression\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 5)\n",
    "        self.linear2 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model_reg = RegressionModel()\n",
    "criterion_reg = nn.MSELoss()\n",
    "optimizer_reg = optim.SGD(model_reg.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_reg = 1000\n",
    "losses_train_reg = []\n",
    "losses_val_reg = []\n",
    "\n",
    "for epoch in range(epochs_reg):\n",
    "    model_reg.train()\n",
    "    epoch_losses_train = []\n",
    "\n",
    "    # Training phase\n",
    "    for inputs, targets in train_loader_reg:\n",
    "        optimizer_reg.zero_grad()\n",
    "        outputs_train = model_reg(inputs)\n",
    "        loss_train = criterion_reg(outputs_train, targets)\n",
    "        loss_train.backward()\n",
    "        optimizer_reg.step()\n",
    "        epoch_losses_train.append(loss_train.item())\n",
    "\n",
    "    # Calculate mean loss for the epoch\n",
    "    losses_train_reg.append(np.mean(epoch_losses_train))\n",
    "\n",
    "    # Evaluation phase (validation)\n",
    "    model_reg.eval()\n",
    "    epoch_losses_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_reg:\n",
    "            outputs_val = model_reg(inputs)\n",
    "            loss_val = criterion_reg(outputs_val, targets)\n",
    "            epoch_losses_val.append(loss_val.item())\n",
    "\n",
    "    # Calculate mean loss for validation set\n",
    "    losses_val_reg.append(np.mean(epoch_losses_val))\n",
    "\n",
    "    # Print progress every 50 epochs\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch [{epoch}/{epochs_reg}], Train Loss: {losses_train_reg[-1]:.4f}, Val Loss: {losses_val_reg[-1]:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses over epochs for regression\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_train_reg, label='Training Loss')\n",
    "plt.plot(losses_val_reg, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Regression Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the learned regression curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(X_reg.numpy(), y_reg.numpy(), color='g', label='Original Data')\n",
    "plt.plot(X_reg.numpy(), model_reg(X_reg).detach().numpy(), color='r', label='Learned Regression')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sin(x)')\n",
    "plt.title('Regression: Original vs Learned Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
