{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics\n",
    "\n",
    "\n",
    "### Introduction to PyTorch Tensors\n",
    "PyTorch tensors are similar to NumPy arrays, but they can also be used on a GPU to accelerate computing. Let's start by creating some tensors and performing basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "print(\"Tensor x:\", x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform operations\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "print(\"Tensor z:\", z)\n",
    "\n",
    "# Compute the mean\n",
    "out = z.mean()\n",
    "print(\"Mean of z:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same thing rewritten as "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform operations\n",
    "y = x + 2\n",
    "z = y * y * 2\n",
    "print(\"Tensor z:\", z)\n",
    "\n",
    "# Compute the mean\n",
    "out = torch.mean(z)\n",
    "print(\"Mean of z:\", out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "a = torch.tensor([10, 20, 30])\n",
    "b = torch.tensor([40,50,60])\n",
    "c = torch.zeros_like(a) #create zero tensor like shape of a\n",
    "\n",
    "print(f\"a: {a}\\nb:{b}\\nc: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just add a and b element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(a.shape[0]):\n",
    "    c[i] = a[i] + b[i]\n",
    "\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instead we will write like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets see back prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# want to find \n",
    "$$\\frac{\\partial c}{\\partial a}, \\frac{\\partial c}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([10.,20.,30.], requires_grad=True)\n",
    "b = torch.tensor([40.,50.,60.], requires_grad=True)\n",
    "\n",
    "c = a + b\n",
    "\n",
    "c.backward() #error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuntimeError: grad can be implicitly created only for scalar outputs\n",
    "\n",
    "This is because z. backward() (z is not scalar). Only scalars can apply .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = c.sum() #for sumup \n",
    "y.backward() # no error why?\n",
    "\n",
    "\n",
    "y #its scalar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial c}{\\partial a}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([10.,20.,30.], requires_grad=True) #only floating point no.s in torch tensorss for grad\n",
    "b = torch.tensor([40.,50.,60.], requires_grad=True) #only floating point no.s in torch tensorss for grad\n",
    "\n",
    "c = a + b\n",
    "\n",
    "z = c.mean()\n",
    "z.backward() #For numerator part\n",
    "\n",
    "print(f\"Gradient wrt a: {a.grad}\\n Gradient wrt b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if requires_grad not True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([10.,20.,30.]) #only floating point no.s in torch tensorss for grad\n",
    "b = torch.tensor([40.,50.,60.]) #only floating point no.s in torch tensorss for grad\n",
    "\n",
    "c = a + b\n",
    "\n",
    "z = c.mean()\n",
    "z.backward() #For numerator part\n",
    "\n",
    "print(f\"Gradient wrt a: {a.grad}\\n Gradient wrt b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn```\n",
    "\n",
    "\n",
    "Must and should **requires_grad=True**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if we call backward for second time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward() #which variable we need to calculate\n",
    "\n",
    "z = a.grad #wrt which variable\n",
    "\n",
    "print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "y = b.grad\n",
    "print(f\"Simmilarly Derivative of c w.r.t b:{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward() #which variable we need to calculate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward. \n",
    "\n",
    "\n",
    "\n",
    "## This is because this error occurs because PyTorch frees the intermediate values of the computational graph after c.backward() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell any no. of times no problem it will run\n",
    "c.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "z = a.grad #wrt which variable\n",
    "\n",
    "print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "y = b.grad\n",
    "print(f\"Simmilarly Derivative of c w.r.t b:{y}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But problem (Gradient accumulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$a = 5, b = 20,  c = a^2 + b^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial c}{\\partial a} = 2a = 10\\\\[10pt]\\frac{\\partial c}{\\partial b} = 2b = 40\\\\[10pt]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"i: {i + 1} \\n\")\n",
    "    c.backward(retain_graph=True)\n",
    "\n",
    "    z = a.grad #wrt which variable\n",
    "\n",
    "    print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "    y = b.grad\n",
    "    print(f\"Simmilarly Derivative of c w.r.t b:{y}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See the output whenever .backward() calls the gradient values get accumulated (added) with previous ones \n",
    "\n",
    "\n",
    "So inorder to avoid use **.grad.zero_()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(f\"i: {i + 1} \\n\")\n",
    "    c.backward(retain_graph=True)\n",
    "\n",
    "    z = a.grad #wrt which variable\n",
    "\n",
    "    print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "    y = b.grad\n",
    "    print(f\"Simmilarly Derivative of c w.r.t b:{y}\\n\")\n",
    "\n",
    "    #clear the grad buffers of a and b\n",
    "    a.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe above two outputs grad values not updated here\n",
    "\n",
    "\n",
    "```We use this trick in Ann_torch.ipynb, gd_types.ipynb files```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conversions\n",
    "\n",
    "While writing codes we need to convert torch from numpy or numpy to torch etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor to numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch to numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = torch.tensor([10.0])\n",
    "\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy\n",
    "\n",
    "b = a.numpy()\n",
    "\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# numpy to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy to torch\n",
    "\n",
    "a = np.array([10])\n",
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conver to torch\n",
    "\n",
    "b = torch.from_numpy(a)\n",
    "\n",
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please gothrough **torch_backprop.ipynb** for more computational graph examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
