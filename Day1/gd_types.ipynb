{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR example\n",
    "\n",
    " X1 | X2| Y \n",
    "----|----|----\n",
    "0|0|0\n",
    "0|1|1\n",
    "1|0|1\n",
    "1|1|0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"images/Ann_1.jpg\" alt=\"Computational Graph\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X =   torch.tensor([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "y = torch.tensor([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "input_layer_neurons = X.shape[1] #index starts with zero\n",
    "hidden_layer_neurons = 3 #Total 3 hidden units\n",
    "output_neuron = 1 #output size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(input_layer_neurons, hidden_layer_neurons, output_neuron):\n",
    "    W1 = torch.nn.Parameter(2 * torch.rand((input_layer_neurons, hidden_layer_neurons)) - 1, requires_grad=True)\n",
    "    b1 = torch.nn.Parameter(2 * torch.rand((1, hidden_layer_neurons)) - 1, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Lets print shape\n",
    "    print(f\"Weights shape from input to hidden layer w_1: {W1.shape}\")\n",
    "    print(f\"Bias shape from input to hidden layer b_1: {b1.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Weights and biases for the hidden to output layer\n",
    "    W2 = torch.nn.Parameter(2 * torch.rand((hidden_layer_neurons, output_neuron)) - 1, requires_grad=True)\n",
    "    b2 = torch.nn.Parameter(2 * torch.rand((1, output_neuron)) - 1, requires_grad=True)\n",
    "\n",
    "    #Lets print shape\n",
    "    print(f\"Weights shape from hidden layer to output layer w_2: {W2.shape}\")\n",
    "    print(f\"Bias shape from hidden layer to output layer b_2: {b2.shape}\")\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def forward_prop(X, W1, b1, W2, b2):\n",
    "    # From input to hidden layer\n",
    "    z1 = torch.matmul(X, W1) + b1\n",
    "    a1 = torch.sigmoid(z1)\n",
    "    z2 = torch.matmul(a1, W2) + b2\n",
    "    a2 = torch.sigmoid(z2)\n",
    "\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "\n",
    "$$\n",
    "W \\leftarrow W - \\eta \\nabla_W L \\\\[10pt]\n",
    "b \\leftarrow b - \\eta \\nabla_b L \\\\[10pt]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialte weights\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "\n",
    "\n",
    "eta = 1e-1\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    z1, a1, z2, a2 = forward_prop(X, W1, b1, W2, b2)\n",
    "\n",
    "    loss = torch.mean((y - a2) ** 2) / 2\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        W1 -= eta * W1.grad\n",
    "        b1 -= eta * b1.grad\n",
    "        W2 -= eta * W2.grad\n",
    "        b2 -= eta * b2.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('XOR Problem GD')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final output\n",
    "_, _, _, final_output = forward_prop(X, W1, b1, W2, b2)\n",
    "predictions = np.where(final_output.detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "print(\"Final predictions:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Momentum based Gradient Descent\n",
    "\n",
    "$$\n",
    "\\text{Initialize: }v_0(W) = v_0(b) = 0\\\\[10pt]\n",
    "v_t(W) =  \\gamma v_{t-1}(W) + \\eta \\nabla_W L \\\\[10pt]\n",
    "W \\leftarrow W- v_t(W)\\\\[10pt]\n",
    "v_t(b) = \\gamma v_{t-1}(b) + \\eta \\nabla_b L \\\\[10pt]\n",
    "b \\leftarrow b-v_t(b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "eta = 1e-1\n",
    "momentum = 0.9\n",
    "losses = []\n",
    "\n",
    "# Initialize velocities\n",
    "v_W1 = torch.zeros_like(W1)\n",
    "v_b1 = torch.zeros_like(b1)\n",
    "v_W2 = torch.zeros_like(W2)\n",
    "v_b2 = torch.zeros_like(b2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    z1, a1, z2, a2 = forward_prop(X, W1, b1, W2, b2)\n",
    "\n",
    "    loss = torch.mean((y - a2) ** 2) / 2\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        # Update velocities\n",
    "        v_W1 = momentum * v_W1 + eta * W1.grad\n",
    "        v_b1 = momentum * v_b1 + eta * b1.grad\n",
    "        v_W2 = momentum * v_W2 + eta * W2.grad\n",
    "        v_b2 = momentum * v_b2 + eta * b2.grad\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= v_W1\n",
    "        b1 -= v_b1\n",
    "        W2 -= v_W2\n",
    "        b2 -= v_b2\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('XOR Problem Momentum')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final output\n",
    "_, _, _, final_output = forward_prop(X, W1, b1, W2, b2)\n",
    "predictions = np.where(final_output.detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "print(\"Final predictions:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nesterov Accelerated Gradient\n",
    "\n",
    "$$\n",
    "\n",
    "\\text{Initialize: }v_0(W) = v_0(b) = 0 \\\\[10pt]\n",
    "\n",
    "v_t(W) =  \\gamma v_{t-1}(W) + \\eta \\nabla_W L(W-\\gamma v_{t-1}(W)) \\\\[10pt]\n",
    "W \\leftarrow W- v_t(W)\\\\[10pt]\n",
    "v_t(b) = \\gamma v_{t-1}(b) + \\eta \\nabla_b L(b-\\gamma v_{t-1}(b)) \\\\[10pt]\n",
    "b \\leftarrow b-v_t(b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "eta = 1e-1\n",
    "momentum = 0.9\n",
    "losses = []\n",
    "\n",
    "# Initialize velocities\n",
    "v_W1 = torch.zeros_like(W1)\n",
    "v_b1 = torch.zeros_like(b1)\n",
    "v_W2 = torch.zeros_like(W2)\n",
    "v_b2 = torch.zeros_like(b2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Lookahead step\n",
    "    W1_lookahead = W1 - momentum * v_W1\n",
    "    b1_lookahead = b1 - momentum * v_b1\n",
    "    W2_lookahead = W2 - momentum * v_W2\n",
    "    b2_lookahead = b2 - momentum * v_b2\n",
    "\n",
    "    z1, a1, z2, a2 = forward_prop(X, W1_lookahead, b1_lookahead, W2_lookahead, b2_lookahead)\n",
    "\n",
    "    loss = torch.mean((y - a2) ** 2) / 2\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        # Update velocities\n",
    "        v_W1 = momentum * v_W1 + eta * W1.grad\n",
    "        v_b1 = momentum * v_b1 + eta * b1.grad\n",
    "        v_W2 = momentum * v_W2 + eta * W2.grad\n",
    "        v_b2 = momentum * v_b2 + eta * b2.grad\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= v_W1\n",
    "        b1 -= v_b1\n",
    "        W2 -= v_W2\n",
    "        b2 -= v_b2\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('XOR Problem Nesterov')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final output\n",
    "_, _, _, final_output = forward_prop(X, W1, b1, W2, b2)\n",
    "predictions = np.where(final_output.detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "print(\"Final predictions:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMS PROP\n",
    "\n",
    "$$\n",
    "\\text{Initialize: }v_0(W) = v_0(b) = 0\\\\[10pt]\n",
    "v_t(W) = \\beta v_{t-1}(W) + (1- \\beta)(\\nabla_w L)^2 \\\\[10pt]\n",
    "W \\leftarrow W - \\frac{\\eta}{\\sqrt{(v_t(W))} + \\epsilon}\\nabla_w(L) \\\\[10pt]\n",
    "v_t(b) = \\beta v_{t-1}(b) + (1- \\beta)(\\nabla_b L)^2 \\\\[10pt]\n",
    "b \\leftarrow b -\\frac{\\eta}{\\sqrt{(v_t(b))} + \\epsilon}\\nabla_b(L) \\\\[10pt]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "# RMSProp parameters\n",
    "beta = 0.9\n",
    "epsilon = 1e-8\n",
    "\n",
    "# Initialize weights and gradients\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "v_W1, v_b1, v_W2, v_b2 = torch.zeros_like(W1), torch.zeros_like(b1), torch.zeros_like(W2), torch.zeros_like(b2)\n",
    "\n",
    "\n",
    "eta = 1e-1\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    z1, a1, z2, a2 = forward_prop(X, W1, b1, W2, b2)\n",
    "\n",
    "    loss = torch.mean((y - a2) ** 2) / 2\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Update the moving averages of the squared gradients\n",
    "        v_W1 = beta * v_W1 + (1 - beta) * W1.grad ** 2\n",
    "        v_b1 = beta * v_b1 + (1 - beta) * b1.grad ** 2\n",
    "        v_W2 = beta * v_W2 + (1 - beta) * W2.grad ** 2\n",
    "        v_b2 = beta * v_b2 + (1 - beta) * b2.grad ** 2\n",
    "\n",
    "        # Update the weights using RMSProp rule\n",
    "        W1 -= eta * W1.grad / (torch.sqrt(v_W1) + epsilon)\n",
    "        b1 -= eta * b1.grad / (torch.sqrt(v_b1) + epsilon)\n",
    "        W2 -= eta * W2.grad / (torch.sqrt(v_W2) + epsilon)\n",
    "        b2 -= eta * b2.grad / (torch.sqrt(v_b2) + epsilon)\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('XOR Problem RMS Prop')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final output\n",
    "_, _, _, final_output = forward_prop(X, W1, b1, W2, b2)\n",
    "predictions = np.where(final_output.detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "print(\"Final predictions:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADAM optimizer (Momentum + RMS Prop)\n",
    "\n",
    "$$\n",
    "\\text{Initialize: }m_0, v_0 = 0\\\\[10pt]\n",
    "m_t(W) = \\beta _1m_{t-1}(W) + (1-\\beta_1)\\nabla_WL \\\\[10pt]\n",
    "v_t(W) = \\beta _2v_{t-1}(W) + (1-\\beta_2)(\\nabla_WL)^2 \\\\[10pt]\n",
    "\\hat{m}_t(W) = \\frac{m_t(W)}{1-\\beta_1^t}\\\\[10pt]\n",
    "\\hat{v}_t(W) = \\frac{v_t(W)}{1-\\beta_2^t}\\\\[10pt]\n",
    "W \\leftarrow W - \\eta \\frac{\\hat{m}_t(W)}{\\sqrt{\\hat{v}_t(W)}+\\epsilon} \\\\[10pt]\n",
    "\n",
    "m_t(b) = \\beta_1 m_{t-1}(b) + (1-\\beta_1)\\nabla_bL \\\\[10pt]\n",
    "v_t(b) = \\beta_2 v_{t-1}(b) + (1-\\beta_2)(\\nabla_b L)^2\\\\[10pt]\n",
    "\n",
    "\\hat{m}_t(b) = \\frac{m_t (b)}{1-\\beta_1^t}\\\\[10pt]\n",
    "\\hat{v}_t(b) = \\frac{v_t (b)}{1-\\beta_2^t}\\\\[10pt]\n",
    "b \\leftarrow b - \\eta \\frac{\\hat{m}_t(b)}{\\sqrt{\\hat{v}_t(b)}+\\epsilon} \\\\[10pt]\n",
    "\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "\n",
    "eta = 1e-3\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "epsilon = 1e-8\n",
    "losses = []\n",
    "\n",
    "# Initialize moment estimates\n",
    "m_W1, v_W1 = torch.zeros_like(W1), torch.zeros_like(W1)\n",
    "m_b1, v_b1 = torch.zeros_like(b1), torch.zeros_like(b1)\n",
    "m_W2, v_W2 = torch.zeros_like(W2), torch.zeros_like(W2)\n",
    "m_b2, v_b2 = torch.zeros_like(b2), torch.zeros_like(b2)\n",
    "\n",
    "for t in range(1, epoch + 1):\n",
    "    z1, a1, z2, a2 = forward_prop(X, W1, b1, W2, b2)\n",
    "\n",
    "    loss = torch.mean((y - a2) ** 2) / 2\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward(retain_graph=True)\n",
    "    with torch.no_grad():\n",
    "        # Update biased first moment estimate\n",
    "        m_W1 = beta1 * m_W1 + (1 - beta1) * W1.grad\n",
    "        m_b1 = beta1 * m_b1 + (1 - beta1) * b1.grad\n",
    "        m_W2 = beta1 * m_W2 + (1 - beta1) * W2.grad\n",
    "        m_b2 = beta1 * m_b2 + (1 - beta1) * b2.grad\n",
    "\n",
    "        # Update biased second moment estimate\n",
    "        v_W1 = beta2 * v_W1 + (1 - beta2) * W1.grad ** 2\n",
    "        v_b1 = beta2 * v_b1 + (1 - beta2) * b1.grad ** 2\n",
    "        v_W2 = beta2 * v_W2 + (1 - beta2) * W2.grad ** 2\n",
    "        v_b2 = beta2 * v_b2 + (1 - beta2) * b2.grad ** 2\n",
    "\n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_hat_W1 = m_W1 / (1 - beta1 ** t)\n",
    "        m_hat_b1 = m_b1 / (1 - beta1 ** t)\n",
    "        m_hat_W2 = m_W2 / (1 - beta1 ** t)\n",
    "        m_hat_b2 = m_b2 / (1 - beta1 ** t)\n",
    "\n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_hat_W1 = v_W1 / (1 - beta2 ** t)\n",
    "        v_hat_b1 = v_b1 / (1 - beta2 ** t)\n",
    "        v_hat_W2 = v_W2 / (1 - beta2 ** t)\n",
    "        v_hat_b2 = v_b2 / (1 - beta2 ** t)\n",
    "\n",
    "        # Update weights\n",
    "        W1 -= eta * m_hat_W1 / (torch.sqrt(v_hat_W1) + epsilon)\n",
    "        b1 -= eta * m_hat_b1 / (torch.sqrt(v_hat_b1) + epsilon)\n",
    "        W2 -= eta * m_hat_W2 / (torch.sqrt(v_hat_W2) + epsilon)\n",
    "        b2 -= eta * m_hat_b2 / (torch.sqrt(v_hat_b2) + epsilon)\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "\n",
    "    if t % 5000 == 0:\n",
    "        print(f\"Epoch {t}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('XOR Problem ADAM')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-0.5, 1.5, 100), np.linspace(-0.5, 1.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Plot the loss over iterations\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over iterations')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "# Final output\n",
    "_, _, _, final_output = forward_prop(X, W1, b1, W2, b2)\n",
    "predictions = np.where(final_output.detach().numpy() > 0.5, 1, 0)\n",
    "\n",
    "print(\"Final predictions:\\n\", predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) \n",
    "\n",
    "Instead of whole data we pass a batch of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset with more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic classification dataset (moons)\n",
    "n_samples = 5000\n",
    "X_cls, y_cls = make_moons(n_samples=n_samples, noise=0.10, random_state=42)\n",
    "X_cls = torch.tensor(X_cls, dtype=torch.float32)\n",
    "y_cls = torch.tensor(y_cls, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train, val, test sets (70%, 20%, 10%)\n",
    "X_train_cls, X_temp_cls, y_train_cls, y_temp_cls = train_test_split(X_cls, y_cls, test_size=0.3, random_state=42)\n",
    "X_val_cls, X_test_cls, y_val_cls, y_test_cls = train_test_split(X_temp_cls, y_temp_cls, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert y_train_cls to numpy for plotting\n",
    "y_train_cls_np = y_train_cls.numpy().ravel()\n",
    "\n",
    "# Plot the training data\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter_0 = plt.scatter(X_train_cls[y_train_cls_np == 0][:, 0], X_train_cls[y_train_cls_np == 0][:, 1], color='blue', label='Class 0', s=10)\n",
    "scatter_1 = plt.scatter(X_train_cls[y_train_cls_np == 1][:, 0], X_train_cls[y_train_cls_np == 1][:, 1], color='red', label='Class 1', s=10)\n",
    "plt.title('Training Data')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.legend(handles=[scatter_0, scatter_1])\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize weights randomly with mean 0\n",
    "input_layer_neurons = X_train_cls.shape[1] #index starts with zero\n",
    "hidden_layer_neurons = 3 #Total 3 hidden units\n",
    "output_neuron = 1 #output size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for batch processing\n",
    "train_dataset_cls = TensorDataset(X_train_cls, y_train_cls)\n",
    "val_dataset_cls = TensorDataset(X_val_cls, y_val_cls)\n",
    "test_dataset_cls = TensorDataset(X_test_cls, y_test_cls)\n",
    "\n",
    "train_loader_cls = DataLoader(train_dataset_cls, batch_size=32, shuffle=True)\n",
    "val_loader_cls = DataLoader(val_dataset_cls, batch_size=32, shuffle=False)\n",
    "test_loader_cls = DataLoader(test_dataset_cls, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(input_layer_neurons, hidden_layer_neurons, output_neuron):\n",
    "    W1 = torch.nn.Parameter(2 * torch.rand((input_layer_neurons, hidden_layer_neurons)) - 1, requires_grad=True)\n",
    "    b1 = torch.nn.Parameter(2 * torch.rand((1, hidden_layer_neurons)) - 1, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Lets print shape\n",
    "    print(f\"Weights shape from input to hidden layer w_1: {W1.shape}\")\n",
    "    print(f\"Bias shape from input to hidden layer b_1: {b1.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "    # Weights and biases for the hidden to output layer\n",
    "    W2 = torch.nn.Parameter(2 * torch.rand((hidden_layer_neurons, output_neuron)) - 1, requires_grad=True)\n",
    "    b2 = torch.nn.Parameter(2 * torch.rand((1, output_neuron)) - 1, requires_grad=True)\n",
    "\n",
    "    #Lets print shape\n",
    "    print(f\"Weights shape from hidden layer to output layer w_2: {W2.shape}\")\n",
    "    print(f\"Bias shape from hidden layer to output layer b_2: {b2.shape}\")\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "def forward_prop(X, W1, b1, W2, b2):\n",
    "    # From input to hidden layer\n",
    "    z1 = torch.matmul(X, W1) + b1\n",
    "    a1 = torch.relu(z1)\n",
    "    z2 = torch.matmul(a1, W2) + b2\n",
    "    a2 = torch.sigmoid(z2)\n",
    "\n",
    "    return z1, a1, z2, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "W1, b1, W2, b2 = initialize(input_layer_neurons, hidden_layer_neurons, output_neuron)\n",
    "\n",
    "# Hyperparameters\n",
    "#epochs = 1000\n",
    "eta = 1e-1\n",
    "losses_train_cls = []\n",
    "losses_val_cls = []\n",
    "accuracies_train_cls = []\n",
    "accuracies_val_cls = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses_train = []\n",
    "    epoch_correct_train = 0\n",
    "    epoch_total_train = 0\n",
    "\n",
    "    # Training phase\n",
    "    for inputs, targets in train_loader_cls:\n",
    "        z1, a1, z2, a2 = forward_prop(inputs, W1, b1, W2, b2)\n",
    "\n",
    "        loss = torch.mean((targets.float().view(-1, 1) - a2) ** 2) / 2\n",
    "        epoch_losses_train.append(loss.item())\n",
    "        loss.backward(retain_graph=True)\n",
    "        with torch.no_grad():\n",
    "            W1 -= eta * W1.grad\n",
    "            b1 -= eta * b1.grad\n",
    "            W2 -= eta * W2.grad\n",
    "            b2 -= eta * b2.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            W1.grad.zero_()\n",
    "            b1.grad.zero_()\n",
    "            W2.grad.zero_()\n",
    "            b2.grad.zero_()\n",
    "\n",
    "        # Calculate training accuracy\n",
    "        predicted_train = (a2 >= 0.5).float()\n",
    "        epoch_correct_train += (predicted_train == targets.float().view_as(predicted_train)).sum().item()\n",
    "        epoch_total_train += targets.size(0)\n",
    "\n",
    "    # Calculate mean loss and accuracy for training set\n",
    "    losses_train_cls.append(np.mean(epoch_losses_train))\n",
    "    accuracy_train = epoch_correct_train / epoch_total_train\n",
    "    accuracies_train_cls.append(accuracy_train)\n",
    "\n",
    "    # Evaluation phase (validation)\n",
    "    epoch_losses_val = []\n",
    "    epoch_correct_val = 0\n",
    "    epoch_total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader_cls:\n",
    "            z1, a1, z2, a2 = forward_prop(inputs, W1, b1, W2, b2)\n",
    "            loss_val = torch.mean((targets.float().view(-1, 1) - a2) ** 2) / 2\n",
    "            epoch_losses_val.append(loss_val.item())\n",
    "\n",
    "            # Calculate validation accuracy\n",
    "            predicted_val = (a2 >= 0.5).float()\n",
    "            epoch_correct_val += (predicted_val == targets.float().view_as(predicted_val)).sum().item()\n",
    "            epoch_total_val += targets.size(0)\n",
    "\n",
    "    # Calculate mean loss and accuracy for validation set\n",
    "    losses_val_cls.append(np.mean(epoch_losses_val))\n",
    "    accuracy_val = epoch_correct_val / epoch_total_val\n",
    "    accuracies_val_cls.append(accuracy_val)\n",
    "\n",
    "    # Print progress every epoch\n",
    "    print(f\"Epoch {epoch}, Train Loss: {losses_train_cls[-1]:.4f}, Val Loss: {losses_val_cls[-1]:.4f}, \"\n",
    "          f\"Train Acc: {accuracy_train:.4f}, Val Acc: {accuracy_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy over iterations side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot for Loss over iterations\n",
    "ax1.plot(losses_train_cls, label='Training Loss')\n",
    "ax1.plot(losses_val_cls, label='Validation Loss')\n",
    "ax1.set_title('Loss over epochs')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot for Accuracy over iterations\n",
    "ax2.plot(accuracies_train_cls, label='Training Accuracy')\n",
    "ax2.plot(accuracies_val_cls, label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy over epochs')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plt.scatter(X_test_cls[:, 0], X_test_cls[:, 1], c=y_test_cls.numpy().ravel(), cmap='viridis', marker='o', s=100, edgecolor='k')\n",
    "plt.title('Moon Problem Test set')\n",
    "\n",
    "# Create a mesh to plot the decision boundary\n",
    "xx, yy = np.meshgrid(np.linspace(-3.5, 3.5, 100), np.linspace(-2.5, 2.5, 100))\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "# Forward pass on the grid\n",
    "_, _, _, a2_grid = forward_prop(grid, W1, b1, W2, b2)\n",
    "a2_grid = a2_grid.detach().numpy().reshape(xx.shape)\n",
    "\n",
    "# Plot the decision boundary\n",
    "plt.contourf(xx, yy, a2_grid, levels=[0, 0.5, 1], alpha=0.2, colors=['blue', 'yellow'])\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "1. Experiment SGD with Momentum, Nesterov momentum and ADAM and plot the decision surfaces\n",
    "1. Observe the key difference in decision surface among optimization strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
