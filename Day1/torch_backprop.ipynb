{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small example for computational graph\n",
    "$$\n",
    " a = 5, b= 20 \\\\[10pt] \\text{ Lets find } c= a^2 + b^2\n",
    " \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find \n",
    "\n",
    "$$\n",
    "\\frac{\\partial c}{\\partial a} = 2a = 2(5.0) = 10.0 \\\\[10pt]\n",
    "\\frac{\\partial c}{\\partial b} = 2b = 2(20.0) = 40.0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward() #which variable we need to calculate\n",
    "\n",
    "z = a.grad #wrt which variable\n",
    "\n",
    "print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "y = b.grad\n",
    "print(f\"Simmilarly Derivative of c w.r.t b:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem when trying to backward through the graph a second time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.backward() #which variable we need to calculate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is because this error occurs because PyTorch frees the intermediate values of the computational graph after c.backward() call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(5.0, requires_grad=True)\n",
    "b = torch.tensor(20.0, requires_grad=True)\n",
    "\n",
    "\n",
    "c = a**2 + b**2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we need to call .backward() multiple times, we should specify retain_graph=True during the first backward call. Hereâ€™s an updated version of the example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c.backward(retain_graph=True)\n",
    "\n",
    "\n",
    "\n",
    "z = a.grad #wrt which variable\n",
    "\n",
    "print(f\"Derivative of c w.r.t a: {z}\")\n",
    "\n",
    "y = b.grad\n",
    "print(f\"Simmilarly Derivative of c w.r.t b:{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/comp_1.jpg\" alt=\"Computational Graph\" style=\"width: 1000px; height: 800px;\">\n",
    "</div>\n",
    "\n",
    "$$\n",
    "a = 3, b=4 \\\\[10pt]\n",
    "c = a*b, \\hspace{5mm}d= a+b, \\hspace{5mm}e= c+d \\\\[10pt]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other examples\n",
    "\n",
    "\n",
    "a = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Define a simple computational graph\n",
    "c = a * b\n",
    "d = a + b\n",
    "e = c + d\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/cg_2.jpg\" alt=\"Computational Graph\" style=\"width: 1000px; height: 1000px;\">\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c}. \\frac{\\partial c}{\\partial a} + \\frac{\\partial e}{\\partial d}. \\frac{\\partial d}{\\partial a} \\hspace{2mm}\n",
    "\n",
    "\\frac{\\partial e}{\\partial c} = 1 + 0 = 1 \\\\[10pt]\n",
    "\\frac{\\partial c}{\\partial a} = 1*b = b = 4 \\\\[10pt]\n",
    "\n",
    "\\frac{\\partial e}{\\partial d} = 0 + 1 = 1\\\\[10pt]\n",
    "\\frac{\\partial d}{\\partial a} = 1 + 0 = 1 \\\\[10pt]\n",
    "\n",
    "\n",
    "\\therefore \\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c}.\\frac{\\partial c}{\\partial a} + \\frac{\\partial e}{\\partial d}.\\frac{\\partial d}{\\partial a} = 1(4) + 1(1) = 5\n",
    "$$\n",
    "\n",
    "Simillarly\n",
    "$$\n",
    "\\frac{\\partial e}{\\partial b} = \\frac{\\partial e}{\\partial c}. \\frac{\\partial c}{\\partial b} + \\frac{\\partial e}{\\partial d} .\\frac{\\partial d}{\\partial b} \\\\[10pt]\n",
    "\n",
    "\\frac{\\partial e}{\\partial c} = 1 + 0 = 1 \\\\[10pt]\n",
    "\\frac{\\partial c}{\\partial b} = a*1 = a = 3 \\\\[10pt]\n",
    "\n",
    "\\frac{\\partial e}{\\partial d} = 0 + 1 = 1\\\\[10pt]\n",
    "\\frac{\\partial d}{\\partial b} = 0+1 = 1 \\\\[10pt]\n",
    "\n",
    "\n",
    "\\therefore \\frac{\\partial e}{\\partial a} = \\frac{\\partial e}{\\partial c}.\\frac{\\partial c}{\\partial a} + \\frac{\\partial e}{\\partial d} .\\frac{\\partial d}{\\partial a} = 1(3) + 1(1) = 4\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward(retain_graph=True)\n",
    "\n",
    "print(f\"Grad w.r.t a: {a.grad}\")\n",
    "print(f\"Grad w.r.t b: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/Exponent_1.jpg\" alt=\"Computational Graph\" style=\"width: 500px; height: 50px;\">\n",
    "</div>\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1+e^{-x}}\\\\[10pt]\n",
    "x = 3, \\hspace{2mm}a = -x,  \\hspace{2mm}b = e^a, \\hspace{2mm}c=1+b, \\hspace{2mm}y=\\frac{1}{c}\\\\[10pt]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "\n",
    "a = -x\n",
    "b = torch.exp(a)\n",
    "c = 1+b\n",
    "y = 1/c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"images/Exponent_2.jpg\" alt=\"Computational Graph\" style=\"width: 500px; height: 250px;\">\n",
    "</div>\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial c}.\\frac{\\partial c}{\\partial b}.\\frac{\\partial b}{\\partial a}.\\frac{\\partial a}{\\partial x} \\\\[10pt]\n",
    "\\frac{\\partial y}{\\partial c} = -\\frac{1}{c^2},\\hspace{2mm} \n",
    "\\frac{\\partial c}{\\partial b} = 1,\\hspace{2mm}\n",
    "\\frac{\\partial b}{\\partial a} = e^a,\\hspace{2mm}\n",
    "\\frac{\\partial a}{\\partial x} = -1 \\\\[10px]\n",
    "\\frac{\\partial y}{\\partial x} = (-\\frac{1}{c^2})(1)(e^a)(-1) \\hspace{2mm}=\n",
    "\n",
    "\\frac{1}{(1+e^{-x})^2}.e^{-x}\\hspace{2mm}= \\frac{1+e^{-x}}{(1+e^{-x})^2}- \\frac{1}{(1+e^{-x})^2}\\\\[10px]\n",
    "\\hspace{10mm} y^1=  y - y^2 = y(1-y)\n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(retain_graph=True)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs and targets\n",
    "inputs = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\n",
    "targets = torch.tensor([[2.0], [6.0]])\n",
    "\n",
    "# Define weights and bias with requires_grad=True\n",
    "weights = torch.tensor([[0.1], [0.2]], requires_grad=True)\n",
    "bias = torch.tensor([0.3], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simple linear model\n",
    "predictions = inputs @ weights + bias # @ is  matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple loss function (Mean Squared Error)\n",
    "loss = torch.mean((predictions - targets) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a backward pass to compute gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the gradients\n",
    "print(f\"inputs: {inputs}\")\n",
    "print(f\"targets: {targets}\")\n",
    "print(f\"weights: {weights}\")\n",
    "print(f\"bias: {bias}\")\n",
    "print(f\"predictions: {predictions}\")\n",
    "print(f\"loss: {loss}\")\n",
    "print(f\"Gradient of loss w.r.t weights: {weights.grad}\")\n",
    "print(f\"Gradient of loss w.r.t bias: {bias.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise with other activation functions like \n",
    "\n",
    "Draw the computation graph and write gradients for every node\n",
    "$$\n",
    "\\text{Tanh}(x) = \\frac{e^x-e^{-x}}{e^x+e^{-x}}\\\\[10pt]\n",
    "\\text{SoftPlus}(x) = \\log(1+e^x)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
